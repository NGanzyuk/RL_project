## Описание проекта

В данном проекте реализованы два алгоритма обучения с подкреплением: SQIL (Soft Q-learning with Imitation Learning) и SAC (Soft Actor-Critic). Проект включает в себя создание и обучение агентов для выполнения задач в окружении LunarLanderContinuous-v2 из библиотеки OpenAI Gym.

## Цели проекта

- Изучение методов обучения с подкреплением.
- Сравнение эффективности SQIL и SAC в задаче управления.
- Визуализация результатов обучения агентов.

## Структура проекта

- QNetwork и PolicyNetwork: классы для создания нейронных сетей, используемых агентами.
- Expert: класс, который генерирует действия на основе заранее заданной политики (в данном случае случайной).
- SQILAgent: класс, реализующий агента, использующего алгоритм SQIL.
- SACAgent: класс, реализующий агента, использующего алгоритм SAC.
- train_sqil_agent и train_sac_agent: функции для обучения SQIL и SAC агентов соответственно.
- plot_results: функция для визуализации результатов обучения.

## Использование

1) Создание окружения: В начале скрипта создается окружение LunarLanderContinuous-v2 с помощью библиотеки gym.

2) Инициализация эксперта: Эксперт генерирует действия, которые будут использоваться для обучения SQIL агента.

3) Обучение SQIL агента: Агента тренируют на основе действий эксперта. Процесс обучения включает добавление переходов в буфер воспроизведения и обновление Q-сети.

4) Обучение SAC агента: Агента тренируют с использованием алгоритма SAC. Он использует две Q-сети и одну сеть политики для оптимизации своих действий.

5) Визуализация результатов: Награды, полученные агентами в процессе обучения, визуализируются на графике для сравнения их эффективности.

## Результаты

После выполнения обучения вы получите график, сравнивающий награды, полученные агентами SQIL и SAC. Это позволит вам оценить, какой из методов обучения работает лучше в данной задаче.